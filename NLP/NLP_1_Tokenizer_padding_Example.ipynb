{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-1.Tokenizer_padding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSTXjohWYP7g",
        "colab_type": "text"
      },
      "source": [
        "# A Simple example of using TensorFlow-Keras Tokenizer and padding feature to convert Sentences into tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArOPfBwyZtln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "241e7883-4926-45ef-8e73-f58729e26f11"
      },
      "source": [
        "# Imports\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# List of sentences\n",
        "sentences = [\n",
        "    'This is the first line',\n",
        "    'This isn\\'t the first line',\n",
        "    'Is this the 3rd line?',\n",
        "    'This line, which is 4th line is too big!?'\n",
        "]\n",
        "\n",
        "# create instance of Tokenizer. SInce \"<OOV>\" is just a simple string. So you can use any unique string instead of this\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")  # add a new token as Out of vocabulary (OOV) for undefined tokens\n",
        "tokenizer.fit_on_texts(sentences)  # encode the passed data\n",
        "word_index = tokenizer.word_index  # convert sentence to a dictionary of word tokens\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences) # convert each sentence to lists of tokens\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=6)  # append/pad zeroes to make all sentences token list to same size\n",
        "\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\\n\", padded)\n",
        "\n",
        "\n",
        "# Try with words that the tokenizer wasn't fit to. \n",
        "test_data = [\n",
        "    'this is really the first line',\n",
        "    'is this the 5th line?'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)   # convert each sentence to lists of tokens\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "padded = pad_sequences(test_seq, maxlen=7)   # append/pad zeroes to make all sentences token list to same size\n",
        "print(\"\\nPadded Test Sequence: \\n\", padded)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'line': 2, 'this': 3, 'is': 4, 'the': 5, 'first': 6, \"isn't\": 7, '3rd': 8, 'which': 9, '4th': 10, 'too': 11, 'big': 12}\n",
            "\n",
            "Sequences =  [[3, 4, 5, 6, 2], [3, 7, 5, 6, 2], [4, 3, 5, 8, 2], [3, 2, 9, 4, 10, 2, 4, 11, 12]]\n",
            "\n",
            "Padded Sequences:\n",
            " [[ 0  3  4  5  6  2]\n",
            " [ 0  3  7  5  6  2]\n",
            " [ 0  4  3  5  8  2]\n",
            " [ 4 10  2  4 11 12]]\n",
            "\n",
            "Test Sequence =  [[3, 4, 1, 5, 6, 2], [4, 3, 5, 1, 2]]\n",
            "\n",
            "Padded Test Sequence: \n",
            " [[0 3 4 1 5 6 2]\n",
            " [0 0 4 3 5 1 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGRJbNKlgOtL",
        "colab_type": "text"
      },
      "source": [
        "## To add padding at the end, Use `padding='post'`. As default paddign is set to \"pre\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qTepbNIadut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "e28ecf20-6d71-4264-a13a-a3ff6f36356d"
      },
      "source": [
        "padded = pad_sequences(test_seq, maxlen=7,padding='post')   # append/pad zeroes to make all sentences token list to same size\n",
        "print(\"\\nPadded Test Sequence: \\n\", padded)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padded Test Sequence: \n",
            " [[3 4 1 5 6 2 0]\n",
            " [4 3 5 1 2 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L18J1uiy6x6",
        "colab_type": "text"
      },
      "source": [
        "## Effect of Removing \"OOV\" (Out of Vocab) token\n",
        "Notice the \"Padded test Sequence\" here. Since we didn't mention the \"<OOV>\" for missing tokens. Therefore, it just ignores the words that are out of the dictionary of tokenized words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz7BVmnUgaTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "6dd0b3b6-069d-4fb8-bb52-7eb635195cff"
      },
      "source": [
        "# Imports\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# List of sentences\n",
        "sentences = [\n",
        "    'This is the first line',\n",
        "    'This isn\\'t the first line',\n",
        "    'Is this the 3rd line?',\n",
        "    'This line, which is 4th line is too big!?'\n",
        "]\n",
        "\n",
        "# create instance of Tokenizer\n",
        "tokenizer = Tokenizer()  # add a new token as Out of vocabulary (OOV) for undefined tokens\n",
        "tokenizer.fit_on_texts(sentences)  # encode the passed data\n",
        "word_index = tokenizer.word_index  # convert sentence to a dictionary of word tokens\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences) # convert each sentence to lists of tokens\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=6)  # append/pad zeroes to make all sentences token list to same size\n",
        "\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\\n\", padded)\n",
        "\n",
        "\n",
        "# Try with words that the tokenizer wasn't fit to. \n",
        "test_data = [\n",
        "    'this is really the first line',\n",
        "    'is this the 5th line?'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)   # convert each sentence to lists of tokens\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "padded = pad_sequences(test_seq, maxlen=7)   # append/pad zeroes to make all sentences token list to same size\n",
        "\n",
        "print(\"\\nPadded Test Sequence: \\n\", padded)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Word Index =  {'line': 1, 'this': 2, 'is': 3, 'the': 4, 'first': 5, \"isn't\": 6, '3rd': 7, 'which': 8, '4th': 9, 'too': 10, 'big': 11}\n",
            "\n",
            "Sequences =  [[2, 3, 4, 5, 1], [2, 6, 4, 5, 1], [3, 2, 4, 7, 1], [2, 1, 8, 3, 9, 1, 3, 10, 11]]\n",
            "\n",
            "Padded Sequences:\n",
            " [[ 0  2  3  4  5  1]\n",
            " [ 0  2  6  4  5  1]\n",
            " [ 0  3  2  4  7  1]\n",
            " [ 3  9  1  3 10 11]]\n",
            "\n",
            "Test Sequence =  [[2, 3, 4, 5, 1], [3, 2, 4, 1]]\n",
            "\n",
            "Padded Test Sequence: \n",
            " [[0 0 2 3 4 5 1]\n",
            " [0 0 0 3 2 4 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Yi3O_EL1OZg",
        "colab_type": "text"
      },
      "source": [
        "## Impact of `Truncating` parameter on the token sequence in 'pad_sequences'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7F7zsLviG3Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "01ece3c3-fc5e-4193-8eee-e2e30e173b20"
      },
      "source": [
        "# Imports\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# List of sentences\n",
        "sentences = [\n",
        "    'This is the first line',\n",
        "    'This isn\\'t the first line',\n",
        "    'Is this the 3rd line?',\n",
        "    'This line, which is 4th line is too big!?'\n",
        "]\n",
        "\n",
        "# create instance of Tokenizer. SInce \"<OOV>\" is just a simple string. So you can use any unique string instead of this\n",
        "tokenizer = Tokenizer(oov_token=\"<OOV>\")  # add a new token as Out of vocabulary (OOV) for undefined tokens\n",
        "tokenizer.fit_on_texts(sentences)  # encode the passed data\n",
        "word_index = tokenizer.word_index  # convert sentence to a dictionary of word tokens\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences) # convert each sentence to lists of tokens\n",
        "\n",
        "padded = pad_sequences(sequences, maxlen=6)  # append/pad zeroes to make all sentences token list to same size\n",
        "\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\\n\", padded)\n",
        "\n",
        "\n",
        "# Try with words that the tokenizer wasn't fit to. \n",
        "test_data = [\n",
        "    'this is really the first line',\n",
        "    'is this the 5th line?'\n",
        "]\n",
        "\n",
        "test_seq = tokenizer.texts_to_sequences(test_data)   # convert each sentence to lists of tokens\n",
        "print(\"\\nTest Sequence = \", test_seq)\n",
        "\n",
        "padded = pad_sequences(test_seq, maxlen=4)   # append/pad zeroes to make all sentences token list to same size\n",
        "\n",
        "# Notice that the sentence token sequence is truncated from the begining beause default value of \"truncation\" paramaeter is \"pre\" \n",
        "print(\"\\nPadded Test Sequence: \\n\", padded)\n",
        "\n",
        "padded = pad_sequences(test_seq, maxlen=4, truncating='post')   # append/pad zeroes to make all sentences token list to same size\n",
        "\n",
        "# Notice that the sentence token sequence is truncated from the end beause we have set value of \"truncating\" paramaeter to \"post\" \n",
        "print(\"\\nPadded Test Sequence: \\n\", padded)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Word Index =  {'<OOV>': 1, 'line': 2, 'this': 3, 'is': 4, 'the': 5, 'first': 6, \"isn't\": 7, '3rd': 8, 'which': 9, '4th': 10, 'too': 11, 'big': 12}\n",
            "\n",
            "Sequences =  [[3, 4, 5, 6, 2], [3, 7, 5, 6, 2], [4, 3, 5, 8, 2], [3, 2, 9, 4, 10, 2, 4, 11, 12]]\n",
            "\n",
            "Padded Sequences:\n",
            " [[ 0  3  4  5  6  2]\n",
            " [ 0  3  7  5  6  2]\n",
            " [ 0  4  3  5  8  2]\n",
            " [ 4 10  2  4 11 12]]\n",
            "\n",
            "Test Sequence =  [[3, 4, 1, 5, 6, 2], [4, 3, 5, 1, 2]]\n",
            "\n",
            "Padded Test Sequence: \n",
            " [[1 5 6 2]\n",
            " [3 5 1 2]]\n",
            "\n",
            "Padded Test Sequence: \n",
            " [[3 4 1 5]\n",
            " [4 3 5 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiWUwv6-1WV3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}