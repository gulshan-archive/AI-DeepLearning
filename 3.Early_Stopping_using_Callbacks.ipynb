{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Description\nFashion_MNIST data and description is available on github: https://github.com/zalandoresearch/fashion-mnist <br>\n\nIt consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes as given below:\n\n**Label \tDescription** <br>\n0 \tT-shirt/top <br>\n1 \tTrouser <br>\n2 \tPullover <br>\n3 \tDress <br>\n4 \tCoat <br>\n5 \tSandal <br>\n6 \tShirt <br>\n7 \tSneaker <br>\n8 \tBag <br>\n9 \tAnkle boot <br>\n\n**Points to be Noted**\n1. Each image is of size 28x28 = 784 pixels.\n2. Grayscale images have a total of 255 shades/pixels ranges from 0 to 255 to define how much dark or light that pixel will be. 0 = Maximum Dark; 255 = Maximum White.\n3. Each row is a separate image and column as labels (0-9)."},{"metadata":{},"cell_type":"markdown","source":"## Callbacks\nUsed for earty stopping of model. for eg. stopping on a apecific onsition like when Loss printed in the logs is less than a specif value or early stopping when model is not able to reduce the loss further "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"#!pip install -U tensorflow==2.0.0-beta1  # to install tensorflow 2.0-beta1 in Kaggle notebook\n# Compile and run with TensorFlow 2.0 \nimport numpy as np\nimport tensorflow as tf\nprint(tf.__version__)\n\n# create a callback class\nclass myCallback(tf.keras.callbacks.Callback):  \n    def on_epoch_end(self, epoch, logs=None):  # on_epoch_end is a standard function defined in tensorflow doc & called at the end of epoch  \n        if(logs.get('loss')<0.3):   # get the 'loss' output from printed logs\n          print(\"\\nReached 70% accuracy so cancelling training!\")\n          self.model.stop_training = True    # standard flag used to stop the model when abov condition meets\n\nOn_epoch_end_callback = myCallback()     # Instantiate the class\n\nmnist = tf.keras.datasets.fashion_mnist\n# Download Fashion mnist data from server\n(x_training_images, y_training_labels), (x_test_images, y_test_labels) = mnist.load_data()\n\n# Normalize the input images to range between 0 to 1\nx_training_images = x_training_images/255.0\nx_test_images = x_test_images/255.0\n\n#Define the model by adding layers\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n#Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n# Train the model\nmodel.fit(x_training_images, y_training_labels, epochs=10, callbacks=[On_epoch_end_callback])\n","execution_count":3,"outputs":[{"output_type":"stream","text":"2.0.0-beta1\nTrain on 60000 samples\nEpoch 1/10\n60000/60000 [==============================] - 7s 115us/sample - loss: 0.4695\nEpoch 2/10\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.3571\nEpoch 3/10\n60000/60000 [==============================] - 6s 108us/sample - loss: 0.3214\nEpoch 4/10\n59616/60000 [============================>.] - ETA: 0s - loss: 0.2996\nReached 70% accuracy so cancelling training!\n60000/60000 [==============================] - 6s 108us/sample - loss: 0.2996\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fecfc411e80>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Another Example of callback: Model will stop training when Loss stops reducing"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Code compile and run with Tsnorflow 2.0\nimport numpy as np\nimport tensorflow as tf\nprint(tf.__version__)\n\n# Callback Class\nclass EarlyStoppingAtMinLoss(tf.keras.callbacks.Callback):\n  \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n  Arguments:\n      patience: Number of epochs to wait after min has been hit. After this\n      number of no improvement, training stops.\n  \"\"\"\n# This method is called when class in Instantiated\n  def __init__(self, patience=0):\n    super(EarlyStoppingAtMinLoss, self).__init__()\n\n    self.patience = patience\n\n    # best_weights to store the weights at which the minimum loss occurs.\n    self.best_weights = None\n\n# This method is called at the begining of training  \n  def on_train_begin(self, logs=None):\n    # The number of epoch it has waited when loss is no longer minimum.\n    self.wait = 0\n    # The epoch the training stops at.\n    self.stopped_epoch = 0\n    # Initialize the best as infinity.\n    self.best = np.Inf\n\n# This method is called on every end of EPOCH     \n  def on_epoch_end(self, epoch, logs=None):\n    current = logs.get('loss')\n    if(current < self.best):\n      self.best = current\n      self.wait = 0\n      # Record the best weights if current results is better (less).\n      self.best_weights = self.model.get_weights()\n    else:\n      self.wait += 1\n      if self.wait >= self.patience:\n        self.stopped_epoch = epoch\n        self.model.stop_training = True   # early stopping of model\n        print('Restoring model weights from the end of the best epoch.')\n        self.model.set_weights(self.best_weights)\n\n# This method is called when training is finished either of its own or by early stopping        \n  def on_train_end(self, logs=None):\n    if self.stopped_epoch > 0:\n      print('Epoch %05d: early stopping' % (self.stopped_epoch + 1))\n\nEarly_stop_callback = EarlyStoppingAtMinLoss()     # Instantiate the class\n\nmnist = tf.keras.datasets.fashion_mnist\n# Downloaing Fashion Mnist data\n(x_training_images, y_training_labels), (x_test_images, y_test_labels) = mnist.load_data()\n\n# Normalize the data in the range 0 to 1\nx_training_images = x_training_images/255.0\nx_test_images = x_test_images/255.0\n#Define the model\nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(),\n  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\nmodel.fit(x_training_images, y_training_labels, epochs=100, callbacks=[Early_stop_callback])","execution_count":8,"outputs":[{"output_type":"stream","text":"2.0.0-beta1\nTrain on 60000 samples\nEpoch 1/100\n60000/60000 [==============================] - 7s 116us/sample - loss: 0.4776\nEpoch 2/100\n60000/60000 [==============================] - 6s 107us/sample - loss: 0.3603\nEpoch 3/100\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.3214\nEpoch 4/100\n60000/60000 [==============================] - 6s 107us/sample - loss: 0.2969\nEpoch 5/100\n60000/60000 [==============================] - 6s 107us/sample - loss: 0.2821\nEpoch 6/100\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.2667\nEpoch 7/100\n60000/60000 [==============================] - 6s 107us/sample - loss: 0.2531\nEpoch 8/100\n60000/60000 [==============================] - 6s 108us/sample - loss: 0.2404\nEpoch 9/100\n60000/60000 [==============================] - 7s 117us/sample - loss: 0.2323\nEpoch 10/100\n60000/60000 [==============================] - 6s 108us/sample - loss: 0.2215\nEpoch 11/100\n60000/60000 [==============================] - 6s 108us/sample - loss: 0.2130\nEpoch 12/100\n60000/60000 [==============================] - 7s 110us/sample - loss: 0.2067\nEpoch 13/100\n60000/60000 [==============================] - 6s 107us/sample - loss: 0.1967\nEpoch 14/100\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.1919\nEpoch 15/100\n60000/60000 [==============================] - 6s 106us/sample - loss: 0.1852\nEpoch 16/100\n60000/60000 [==============================] - 6s 108us/sample - loss: 0.1805\nEpoch 17/100\n60000/60000 [==============================] - 6s 104us/sample - loss: 0.1739\nEpoch 18/100\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.1683\nEpoch 19/100\n60000/60000 [==============================] - 6s 106us/sample - loss: 0.1622\nEpoch 20/100\n60000/60000 [==============================] - 7s 111us/sample - loss: 0.1588\nEpoch 21/100\n60000/60000 [==============================] - 6s 106us/sample - loss: 0.1529\nEpoch 22/100\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.1490\nEpoch 23/100\n60000/60000 [==============================] - 6s 103us/sample - loss: 0.1452\nEpoch 24/100\n60000/60000 [==============================] - 6s 104us/sample - loss: 0.1435\nEpoch 25/100\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.1365\nEpoch 26/100\n60000/60000 [==============================] - 6s 107us/sample - loss: 0.1338\nEpoch 27/100\n60000/60000 [==============================] - 6s 106us/sample - loss: 0.1315\nEpoch 28/100\n60000/60000 [==============================] - 6s 104us/sample - loss: 0.1296\nEpoch 29/100\n60000/60000 [==============================] - 6s 107us/sample - loss: 0.1263\nEpoch 30/100\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.1203\nEpoch 31/100\n59584/60000 [============================>.] - ETA: 0s - loss: 0.1207Restoring model weights from the end of the best epoch.\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.1207\nEpoch 00031: early stopping\n","name":"stdout"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fecfc76e8d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}