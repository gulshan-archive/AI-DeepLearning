{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "4.Simple_CNN_Fashion_Mnist.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKPtajeqUNMc",
        "colab_type": "text"
      },
      "source": [
        "## Convolutional Neural Network Example using Fashion Mnist Dataset\n",
        "Fashion_MNIST data and description is available on github: https://github.com/zalandoresearch/fashion-mnist <br>\n",
        "\n",
        "It consists of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes as given below:\n",
        "\n",
        "**Label \tDescription** <br>\n",
        "0 \tT-shirt/top <br>\n",
        "1 \tTrouser <br>\n",
        "2 \tPullover <br>\n",
        "3 \tDress <br>\n",
        "4 \tCoat <br>\n",
        "5 \tSandal <br>\n",
        "6 \tShirt <br>\n",
        "7 \tSneaker <br>\n",
        "8 \tBag <br>\n",
        "9 \tAnkle boot <br>\n",
        "\n",
        "**Points to be Noted**\n",
        "1. Each image is of size 28x28 = 784 pixels.\n",
        "2. Grayscale images have a total of 255 shades/pixels ranges from 0 to 255 to define how much dark or light that pixel will be. 0 = Maximum Dark; 255 = Maximum White.\n",
        "3. Each row is a separate image and column as labels (0-9).\n",
        "3. Convolutional Neural Network(CNN) is used for dealing with images (computer vision) and increases the accuracy of model a lot even > 99% by finding the edges in the image\n",
        "\n",
        "<table>\n",
        "  <tr><td>\n",
        "    <img src=\"https://tensorflow.org/images/fashion-mnist-sprite.png\"\n",
        "         alt=\"Fashion MNIST sprite\" width=\"600\">\n",
        "  </td></tr>\n",
        "  <tr><td align=\"center\">\n",
        "    <b>Figure 1.</b> <a href=\"https://github.com/zalandoresearch/fashion-mnist\">Fashion-MNIST samples</a> (by Zalando, MIT License).<br/>&nbsp;\n",
        "  </td></tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ru9WcWgtUNMj",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "952bNlMrUNMn",
        "colab_type": "code",
        "colab": {},
        "outputId": "42ce6ca0-891e-443d-c0a1-1f491df8be9b"
      },
      "source": [
        "# Model run with TensorFlow 2.0\n",
        "# !pip install -U tensorflow==2.0.0-beta1  # to install tensorflow 2.0-beta1 in Kaggle notebook\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-rc0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SId0DghFUNM1",
        "colab_type": "text"
      },
      "source": [
        "## Load Training & Test Data and Normalize it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXa3QO2KUNM4",
        "colab_type": "code",
        "colab": {},
        "outputId": "6bea8db5-7f66-43b9-cc96-300465754163"
      },
      "source": [
        "# Load Fashion MNIST Data\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(x_train_images, y_train_labels), (x_test_images, y_test_labels) = mnist.load_data()\n",
        "print(x_train_images.shape)\n",
        "print(y_train_labels.shape)\n",
        "# Convert the image into a 4D list\n",
        "x_train_images = x_train_images.reshape(60000, 28, 28, 1)  # last 1 indicates that its a gray scale image\n",
        "print(x_train_images.shape)\n",
        "x_train_images = x_train_images / 255.0  # Normalize images into values ranging from 0 to 1 and conver to float by dividing it with a float\n",
        "x_test_images = x_test_images.reshape(10000, 28, 28, 1)\n",
        "x_test_images = x_test_images/255.0     # Normalize images into values ranging from 0 to 1 d convert to float by dividing it with a float"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(60000,)\n",
            "(60000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Uw7biuUNND",
        "colab_type": "text"
      },
      "source": [
        "## Define, Compile and Run the Model\n",
        "**Conv2d:** Used to find edges and filter out important information from the image. The number of convolutions in purely arbitrary, but good to start with something in the order of 32 <br>\n",
        "**Maxpool2D:** used to compress the image to half while maintaining the content of the features that were highlighted by the convolution layer (conv2d) <br> <br>\n",
        "**Loss function** — An algorithm for measuring how far the model's outputs are from the desired output. The goal of training is this measures loss.\n",
        "**Optimizer** —An algorithm for adjusting the inner parameters of the model in order to minimize loss.\n",
        "**Metrics** —Used to monitor the training and testing steps.\n",
        "\n",
        "**Notice** that the output size of image is 2x2 pixels less which is 26x26 in comparison to input image size of 28x28. This is because when applying a 3x3 filter. It requires adjacent pixels on all 4 directions (left,right,up,down). But since when the filter is convolved at the uppermost, lowermost, leftmost and rightmost pixels, there is no adjacent pixels. Therefore, it decreases the size by 2 from each size resulting in 26x26 image size. Similarly for a 5x5 filter, the image size will be 24x24"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "jT5cSHiMUNNG",
        "colab_type": "code",
        "colab": {},
        "outputId": "240a91c2-6486-429b-f585-af42bff4ec8e"
      },
      "source": [
        "# Define the Model\n",
        "model = tf.keras.models.Sequential([\n",
        "tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1)),  # add a convolution layer with 64 random filters of size 3,3\n",
        "tf.keras.layers.MaxPooling2D(pool_size=(2,2)), \n",
        "tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "tf.keras.layers.MaxPooling2D(2,2),\n",
        "tf.keras.layers.Flatten(),   # Flatten the image into a single 1D array\n",
        "# Fully Connected Layer\n",
        "tf.keras.layers.Dense(128, activation='relu'),  # Add a neural network layer with 128 neurons\n",
        "# Softmax Layer\n",
        "tf.keras.layers.Dense(10, activation='softmax')]) # Add the final neural network layer with 10 neurons (for 10 classes)\n",
        "\n",
        "\"\"\"Use CategoricalCrossentropy loss function when there are two or more label classes. We expect labels to be \n",
        "provided in a one_hot representation. If you want to provide labels as integers, please use \n",
        "SparseCategoricalCrossentropy loss.\"\"\"\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# to see the size and shape of the network,\n",
        "model.summary()  \n",
        "    \n",
        "model.fit(x_train_images, y_train_labels, epochs=5)\n",
        "\n",
        "test_loss = model.evaluate(x_test_images, y_test_labels, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 243,786\n",
            "Trainable params: 243,786\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 188s 3ms/sample - loss: 0.4281 - accuracy: 0.8449\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 175s 3ms/sample - loss: 0.2871 - accuracy: 0.8944\n",
            "Epoch 3/5\n",
            "11296/60000 [====>.........................] - ETA: 2:16 - loss: 0.2383 - accuracy: 0.9106"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgOjhdVuUNNP",
        "colab_type": "text"
      },
      "source": [
        "## Visualizing the Convolutions and Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Unh7WeqUNNS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_test_labels[:100])  # print first 100 test labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOQo7AnSUNNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,4)   # args: No. of rows, no. of columns\n",
        "# Macros. Add the image number forIst, 2nd and 3rd by checking that which 3 test label represents the same label or class\n",
        "FIRST_IMAGE = 2\n",
        "SECOND_IMAGE = 3\n",
        "THIRD_IMAGE = 5\n",
        "CONVOLUTION_NUMBER = 1\n",
        "\n",
        "from tensorflow.keras import models\n",
        "\n",
        "# Below code can be made in just single line using list comprehension\n",
        "# layer_outputs = []\n",
        "# for layer in model.layers:\n",
        "#     layer_outputs.append(layer.output)\n",
        "# print(layer_outputs)\n",
        "# print('\\n')\n",
        "\n",
        "# List comprehension code of above commented code lines\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "# print(layer_outputs)\n",
        "\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(x_test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "\n",
        "  f2 = activation_model.predict(x_test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "    \n",
        "  f3 = activation_model.predict(x_test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gji_TFX8UNNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}